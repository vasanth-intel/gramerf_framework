properties([
    parameters([
        choice(choices: ['source', 'package'], name: 'build_gramine'),
        string(defaultValue: 'https://github.com/gramineproject/gramine.git', name: 'gramine_repo'),
        string(defaultValue: 'master', name: 'gramine_commit'),
        string(defaultValue: 'https://github.com/gramineproject/gsc.git', name: 'gsc_repo'),
        string(defaultValue: 'master', name: 'gsc_commit'),
        string(defaultValue: '10', name: 'iterations'),
        booleanParam('edmm')
    ])
])

def get_perf_config(workload) {
    container_workloads = ["tf_serving_perf", "pytorch_perf", "ovms_perf", "mysql_perf", "mariadb_perf"]
    if (workload in container_workloads){
        return 'container'
    }
    else{
        return 'baremetal'
    }
}

def get_exec_mode(workload) {
    if (workload in ['mariadb_perf', 'mysql_perf', 'ovms_perf', 'tf_serving_perf']){
        return 'native,gramine-sgx'
    } else{
        return 'native,gramine-direct,gramine-sgx'
    }
}

def get_iterations(workload) {
    if (workload == 'ovms_perf'){
        return '20'
    } else if (workload == 'pytorch_perf'){
        return '3'
    } else if (workload in ['specpower_perf', 'mongodb_perf']){
        return '5'
    } else{
        return iterations
    }
}

pipeline{
    agent {
        label 'JK_POOL_MASTER'
    }
    stages{
        stage(""){
            parallel{
                stage("Build specpower_perf"){
                    steps{
                        script{
                            cleanWs()
                            def spec_node_workload = ['specpower_perf', 'mongodb_perf']
                            spec_node_workload.each{ workload ->
                                build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm).call()
                            }
                        }
                    }
                }
                stage(""){
                    steps{
                        script{
                            def sklearn_node_workloads = ['sklearnex_perf','nginx_perf', 'mysql_perf', 'mariadb_perf']
                            sklearn_node_workloads.each { workload ->
                                if (workload in ['mysql_perf', 'mariadb_perf']){
                                    build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm, [string(name: 'encryption', value: 'true'), string(name: 'tmpfs', value: 'true')]).call() 
                                } else {
                                    build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm).call()
                                }
                                workload = 'mysql_perf'
                                build_perf_job(workload, 'baremetal', get_exec_mode(workload), get_iterations(workload), edmm, [string(name: 'encryption', value: 'true')]).call()
                            }  
                        }
                    }
                }
                stage(" "){
                    steps{
                        script{
                            def node_workloads = ['redis_perf', 'memcached_perf']
                            node_workloads.each{ workload ->
                                build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm).call()
                            }
                            parallel  redisnode: {
                                def redis_node_workloads = ['tf_perf', 'ovms_perf', 'tf_serving_perf']
                                redis_node_workloads.each { workload ->
                                    if (workload in ['ovms_perf']){
                                        build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm, [string(name: 'encryption', value: 'true'), string(name: 'node_name', value: 'graphene_perf_redis_taken_out_for_vasanth')]).call()
                                    } else{
                                        build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm, [string(name: 'node_name', value: 'graphene_perf_redis_taken_out_for_vasanth')]).call()
                                    }                                    
                                }
                                build_perf_job('tf_perf', get_perf_config('tf_perf'), get_exec_mode('tf_perf'), get_iterations('tf_perf'), edmm, [string(name: 'encryption', value: 'true'), string(name: 'node_name', value: 'graphene_perf_redis_taken_out_for_vasanth')]).call() 
                            }, openvinonode: {
                                def openvino_node_workloads = ['ov_perf_latency', 'ov_perf_throughput', 'pytorch_perf']
                                openvino_node_workloads.each { workload ->
                                    build_perf_job(workload, get_perf_config(workload), get_exec_mode(workload), get_iterations(workload), edmm).call()                  
                                }
                                if (!edmm.toBoolean()){
                                    build_perf_job('ov_perf_latency', get_perf_config('ov_perf_latency'), get_exec_mode('ov_perf_latency'), get_iterations('ov_perf_latency'), true).call()
                                    build_perf_job('ov_perf_throughput', get_perf_config('ov_perf_throughput'), get_exec_mode('ov_perf_throughput'), get_iterations('ov_perf_throughput'), true).call()
                                }
                            }                   
                        }
                    }
                }
            }
            post {
                always{
                    script{
                        sh """
                            mkdir -p ${BUILD_TAG}/sklearn_reports
                            cp -r artifacts/results/*.xlsx ${BUILD_TAG}
                            cp -r artifacts/sklearn_reports/results/*.xlsx ${BUILD_TAG}/sklearn_reports
                        """
                        def status = bat returnStatus: true, script: "robocopy ${BUILD_TAG} Z:/gramerf_biweekly_reports/${BUILD_TAG} /E"
                        println "ROBOCOPY returned ${status}"
                        archiveArtifacts artifacts: "${BUILD_TAG}/**", followSymlinks: false
                    }
                }
            }
        }
    }
}

def build_perf_job(workload, perf_config, exec_mode, iterations, is_edmm, extra_params=null) {
    def params = [string(name: 'perf_config', value: perf_config), string(name: 'gramine_repo', value: gramine_repo),
    string(name: 'build_gramine', value: build_gramine), string(name: 'gsc_repo', value: gsc_repo),
    string(name: 'gramine_commit', value: gramine_commit), string(name: 'run', value: workload), 
    string(name: 'gsc_commit', value: gsc_commit), string(name: 'exec_mode', value: exec_mode),
    string(name: 'iterations', value: iterations)]
    if (is_edmm){
        params.add(booleanParam(name: 'edmm', value: is_edmm))
    }
    if (extra_params) {
		params = params + extra_params
	}
    return {
        stage("Build ${workload} with edmm enabled: ${is_edmm}") {
            echo "$params"
            def job = build propagate: false, job: 'gramerf_performance_benchmarking', parameters: params
            if(job.getResult() == 'SUCCESS'){
                dir ('artifacts'){
                    copyArtifacts projectName: 'gramerf_performance_benchmarking', selector: specific("${job.number}")
                }
            }
        }
    }
}